{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)\n",
    "\n",
    "probs = linear_classifer.softmax(np.array([[1000, 0, 0],\n",
    "                                          [1, 2, 5]]))\n",
    "assert np.isclose(probs[0, 0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0486305974370773"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.414366407991502"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 2, 5]))\n",
    "display(linear_classifer.cross_entropy_loss(probs, 1))\n",
    "\n",
    "probs = linear_classifer.softmax(np.array([[-5, 0, 5],\n",
    "                                          [2, 0, 1]]))\n",
    "display(linear_classifer.cross_entropy_loss(probs, [1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 24985.033995\n",
      "Epoch 1, loss: 26486.274974\n",
      "Epoch 2, loss: 25922.768044\n",
      "Epoch 3, loss: 27557.151243\n",
      "Epoch 4, loss: 26840.451264\n",
      "Epoch 5, loss: 27066.749694\n",
      "Epoch 6, loss: 26616.494448\n",
      "Epoch 7, loss: 25624.808225\n",
      "Epoch 8, loss: 25489.122039\n",
      "Epoch 9, loss: 27247.364579\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22e59582948>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.164\n",
      "Epoch 0, loss: 25772.929141\n",
      "Epoch 1, loss: 26644.105777\n",
      "Epoch 2, loss: 26928.544574\n",
      "Epoch 3, loss: 25979.446072\n",
      "Epoch 4, loss: 26707.798208\n",
      "Epoch 5, loss: 27003.303356\n",
      "Epoch 6, loss: 25521.322173\n",
      "Epoch 7, loss: 27493.334627\n",
      "Epoch 8, loss: 27285.695643\n",
      "Epoch 9, loss: 28392.878804\n",
      "Epoch 10, loss: 25988.855168\n",
      "Epoch 11, loss: 26365.625447\n",
      "Epoch 12, loss: 26715.406295\n",
      "Epoch 13, loss: 25124.796715\n",
      "Epoch 14, loss: 27380.860037\n",
      "Epoch 15, loss: 26001.073302\n",
      "Epoch 16, loss: 26225.619902\n",
      "Epoch 17, loss: 26845.792938\n",
      "Epoch 18, loss: 27122.710528\n",
      "Epoch 19, loss: 27542.020215\n",
      "Epoch 20, loss: 26639.728220\n",
      "Epoch 21, loss: 27847.545032\n",
      "Epoch 22, loss: 26235.856837\n",
      "Epoch 23, loss: 26544.309023\n",
      "Epoch 24, loss: 26226.719986\n",
      "Epoch 25, loss: 27890.642027\n",
      "Epoch 26, loss: 27215.284818\n",
      "Epoch 27, loss: 26940.954323\n",
      "Epoch 28, loss: 27394.233812\n",
      "Epoch 29, loss: 24588.589820\n",
      "Epoch 30, loss: 26360.424170\n",
      "Epoch 31, loss: 28060.875933\n",
      "Epoch 32, loss: 25552.523887\n",
      "Epoch 33, loss: 27897.642346\n",
      "Epoch 34, loss: 26022.911748\n",
      "Epoch 35, loss: 26944.505610\n",
      "Epoch 36, loss: 25899.499318\n",
      "Epoch 37, loss: 28158.930738\n",
      "Epoch 38, loss: 27340.438023\n",
      "Epoch 39, loss: 27948.350909\n",
      "Epoch 40, loss: 26802.710021\n",
      "Epoch 41, loss: 26165.188606\n",
      "Epoch 42, loss: 26742.750358\n",
      "Epoch 43, loss: 26600.845091\n",
      "Epoch 44, loss: 27279.192231\n",
      "Epoch 45, loss: 26813.944272\n",
      "Epoch 46, loss: 26794.698138\n",
      "Epoch 47, loss: 27548.694144\n",
      "Epoch 48, loss: 26854.593460\n",
      "Epoch 49, loss: 25488.994772\n",
      "Epoch 50, loss: 25533.116761\n",
      "Epoch 51, loss: 27751.368644\n",
      "Epoch 52, loss: 26152.943612\n",
      "Epoch 53, loss: 27613.340168\n",
      "Epoch 54, loss: 26525.903166\n",
      "Epoch 55, loss: 26134.898801\n",
      "Epoch 56, loss: 25990.896878\n",
      "Epoch 57, loss: 25579.457512\n",
      "Epoch 58, loss: 28123.430558\n",
      "Epoch 59, loss: 25713.499596\n",
      "Epoch 60, loss: 26276.263677\n",
      "Epoch 61, loss: 25442.027325\n",
      "Epoch 62, loss: 26245.550972\n",
      "Epoch 63, loss: 27381.267274\n",
      "Epoch 64, loss: 26840.855023\n",
      "Epoch 65, loss: 27558.087371\n",
      "Epoch 66, loss: 25533.619082\n",
      "Epoch 67, loss: 26934.744918\n",
      "Epoch 68, loss: 25701.895661\n",
      "Epoch 69, loss: 27777.410098\n",
      "Epoch 70, loss: 27519.004405\n",
      "Epoch 71, loss: 25736.365962\n",
      "Epoch 72, loss: 27020.928444\n",
      "Epoch 73, loss: 26880.037292\n",
      "Epoch 74, loss: 26416.843293\n",
      "Epoch 75, loss: 26313.895409\n",
      "Epoch 76, loss: 26891.709475\n",
      "Epoch 77, loss: 27554.053321\n",
      "Epoch 78, loss: 27202.680723\n",
      "Epoch 79, loss: 26269.802701\n",
      "Epoch 80, loss: 28032.887291\n",
      "Epoch 81, loss: 26772.839280\n",
      "Epoch 82, loss: 26956.299008\n",
      "Epoch 83, loss: 26135.112216\n",
      "Epoch 84, loss: 27939.648306\n",
      "Epoch 85, loss: 25644.687135\n",
      "Epoch 86, loss: 27373.496601\n",
      "Epoch 87, loss: 27030.749271\n",
      "Epoch 88, loss: 27771.183574\n",
      "Epoch 89, loss: 27906.484099\n",
      "Epoch 90, loss: 26304.460121\n",
      "Epoch 91, loss: 27266.016921\n",
      "Epoch 92, loss: 27070.816022\n",
      "Epoch 93, loss: 26652.566006\n",
      "Epoch 94, loss: 26680.215025\n",
      "Epoch 95, loss: 26528.351302\n",
      "Epoch 96, loss: 26633.476461\n",
      "Epoch 97, loss: 24248.536119\n",
      "Epoch 98, loss: 28105.920673\n",
      "Epoch 99, loss: 25168.756120\n",
      "Accuracy after training for 100 epochs:  0.173\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0.001, 0.001, 32\n",
      "Epoch 10, loss: 19074.015004\n",
      "Epoch 20, loss: 18812.682117\n",
      "Epoch 30, loss: 18641.429048\n",
      "Epoch 40, loss: 18519.747820\n",
      "Epoch 50, loss: 18418.784818\n",
      "Model 0.001, 0.001, 64\n",
      "Epoch 10, loss: 19103.396045\n",
      "Epoch 20, loss: 18855.753495\n",
      "Epoch 30, loss: 18660.172261\n",
      "Epoch 40, loss: 18531.642051\n",
      "Epoch 50, loss: 18429.900106\n",
      "Model 0.001, 0.001, 128\n",
      "Epoch 10, loss: 19166.542345\n",
      "Epoch 20, loss: 18962.651599\n",
      "Epoch 30, loss: 18863.081794\n",
      "Epoch 40, loss: 18673.844952\n",
      "Epoch 50, loss: 18576.065003\n",
      "Model 0.001, 0.00055, 32\n",
      "Epoch 10, loss: 19085.937120\n",
      "Epoch 20, loss: 18802.702300\n",
      "Epoch 30, loss: 18623.707249\n",
      "Epoch 40, loss: 18494.789499\n",
      "Epoch 50, loss: 18385.372411\n",
      "Model 0.001, 0.00055, 64\n",
      "Epoch 10, loss: 19111.154201\n",
      "Epoch 20, loss: 18835.880981\n",
      "Epoch 30, loss: 18639.928901\n",
      "Epoch 40, loss: 18527.260623\n",
      "Epoch 50, loss: 18389.742263\n",
      "Model 0.001, 0.00055, 128\n",
      "Epoch 10, loss: 19208.306531\n",
      "Epoch 20, loss: 18995.056434\n",
      "Epoch 30, loss: 18695.074021\n",
      "Epoch 40, loss: 18710.019423\n",
      "Epoch 50, loss: 18525.453537\n",
      "Model 0.001, 0.0001, 32\n",
      "Epoch 10, loss: 19063.108903\n",
      "Epoch 20, loss: 18812.297674\n",
      "Epoch 30, loss: 18641.093596\n",
      "Epoch 40, loss: 18489.377108\n",
      "Epoch 50, loss: 18379.510087\n",
      "Model 0.001, 0.0001, 64\n",
      "Epoch 10, loss: 19089.250164\n",
      "Epoch 20, loss: 18858.196688\n",
      "Epoch 30, loss: 18677.411229\n",
      "Epoch 40, loss: 18523.232606\n",
      "Epoch 50, loss: 18427.388294\n",
      "Model 0.001, 0.0001, 128\n",
      "Epoch 10, loss: 19247.220285\n",
      "Epoch 20, loss: 18918.213211\n",
      "Epoch 30, loss: 18734.272682\n",
      "Epoch 40, loss: 18652.208031\n",
      "Epoch 50, loss: 18523.220679\n",
      "Model 0.00055, 0.001, 32\n",
      "Epoch 10, loss: 19154.615957\n",
      "Epoch 20, loss: 18888.057204\n",
      "Epoch 30, loss: 18740.943486\n",
      "Epoch 40, loss: 18630.491444\n",
      "Epoch 50, loss: 18542.873410\n",
      "Model 0.00055, 0.001, 64\n",
      "Epoch 10, loss: 19155.591856\n",
      "Epoch 20, loss: 18901.384248\n",
      "Epoch 30, loss: 18740.034217\n",
      "Epoch 40, loss: 18639.692307\n",
      "Epoch 50, loss: 18534.508743\n",
      "Model 0.00055, 0.001, 128\n",
      "Epoch 10, loss: 19155.244645\n",
      "Epoch 20, loss: 18925.377131\n",
      "Epoch 30, loss: 18759.049615\n",
      "Epoch 40, loss: 18643.181903\n",
      "Epoch 50, loss: 18545.167967\n",
      "Model 0.00055, 0.00055, 32\n",
      "Epoch 10, loss: 19147.948002\n",
      "Epoch 20, loss: 18883.448106\n",
      "Epoch 30, loss: 18740.802156\n",
      "Epoch 40, loss: 18630.258448\n",
      "Epoch 50, loss: 18531.859999\n",
      "Model 0.00055, 0.00055, 64\n",
      "Epoch 10, loss: 19152.295266\n",
      "Epoch 20, loss: 18906.834452\n",
      "Epoch 30, loss: 18727.557006\n",
      "Epoch 40, loss: 18630.524596\n",
      "Epoch 50, loss: 18550.721448\n",
      "Model 0.00055, 0.00055, 128\n",
      "Epoch 10, loss: 19168.319827\n",
      "Epoch 20, loss: 18924.750846\n",
      "Epoch 30, loss: 18775.267745\n",
      "Epoch 40, loss: 18630.225529\n",
      "Epoch 50, loss: 18556.222793\n",
      "Model 0.00055, 0.0001, 32\n",
      "Epoch 10, loss: 19151.022681\n",
      "Epoch 20, loss: 18888.589159\n",
      "Epoch 30, loss: 18731.242685\n",
      "Epoch 40, loss: 18623.487854\n",
      "Epoch 50, loss: 18524.265370\n",
      "Model 0.00055, 0.0001, 64\n",
      "Epoch 10, loss: 19152.830104\n",
      "Epoch 20, loss: 18891.382344\n",
      "Epoch 30, loss: 18743.439693\n",
      "Epoch 40, loss: 18628.962986\n",
      "Epoch 50, loss: 18534.519674\n",
      "Model 0.00055, 0.0001, 128\n",
      "Epoch 10, loss: 19171.348889\n",
      "Epoch 20, loss: 18914.297075\n",
      "Epoch 30, loss: 18754.383345\n",
      "Epoch 40, loss: 18636.529271\n",
      "Epoch 50, loss: 18551.087793\n",
      "Model 0.0001, 0.001, 32\n",
      "Epoch 10, loss: 19676.797174\n",
      "Epoch 20, loss: 19372.130758\n",
      "Epoch 30, loss: 19216.520441\n",
      "Epoch 40, loss: 19110.192204\n",
      "Epoch 50, loss: 19031.020603\n",
      "Model 0.0001, 0.001, 64\n",
      "Epoch 10, loss: 19681.251579\n",
      "Epoch 20, loss: 19372.316671\n",
      "Epoch 30, loss: 19216.329881\n",
      "Epoch 40, loss: 19111.089595\n",
      "Epoch 50, loss: 19030.928177\n",
      "Model 0.0001, 0.001, 128\n",
      "Epoch 10, loss: 19680.918682\n",
      "Epoch 20, loss: 19371.524889\n",
      "Epoch 30, loss: 19214.882388\n",
      "Epoch 40, loss: 19107.455947\n",
      "Epoch 50, loss: 19027.196381\n",
      "Model 0.0001, 0.00055, 32\n",
      "Epoch 10, loss: 19682.421393\n",
      "Epoch 20, loss: 19371.718283\n",
      "Epoch 30, loss: 19217.851541\n",
      "Epoch 40, loss: 19109.847989\n",
      "Epoch 50, loss: 19027.782236\n",
      "Model 0.0001, 0.00055, 64\n",
      "Epoch 10, loss: 19682.970918\n",
      "Epoch 20, loss: 19377.245018\n",
      "Epoch 30, loss: 19215.174085\n",
      "Epoch 40, loss: 19112.984196\n",
      "Epoch 50, loss: 19027.835324\n",
      "Model 0.0001, 0.00055, 128\n",
      "Epoch 10, loss: 19679.913830\n",
      "Epoch 20, loss: 19371.833998\n",
      "Epoch 30, loss: 19217.232736\n",
      "Epoch 40, loss: 19110.731649\n",
      "Epoch 50, loss: 19020.643935\n",
      "Model 0.0001, 0.0001, 32\n",
      "Epoch 10, loss: 19683.746383\n",
      "Epoch 20, loss: 19369.388956\n",
      "Epoch 30, loss: 19213.654347\n",
      "Epoch 40, loss: 19111.039741\n",
      "Epoch 50, loss: 19029.155378\n",
      "Model 0.0001, 0.0001, 64\n",
      "Epoch 10, loss: 19682.610442\n",
      "Epoch 20, loss: 19371.246136\n",
      "Epoch 30, loss: 19216.442201\n",
      "Epoch 40, loss: 19108.243481\n",
      "Epoch 50, loss: 19030.646647\n",
      "Model 0.0001, 0.0001, 128\n",
      "Epoch 10, loss: 19682.334128\n",
      "Epoch 20, loss: 19374.181881\n",
      "Epoch 30, loss: 19217.085361\n",
      "Epoch 40, loss: 19107.883375\n",
      "Epoch 50, loss: 19029.992251\n",
      "best validation accuracy achieved: 0.253000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = np.linspace(0.001, 0.0001, 3)\n",
    "reg_strengths = np.linspace(0.001, 0.0001, 3)\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for learning_rate in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Model {learning_rate}, {reg}, {batch_size}\")\n",
    "            classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "            classifier.fit(train_X, train_y, epochs=50, learning_rate=learning_rate, batch_size=batch_size, reg=reg)\n",
    "            pred = classifier.predict(val_X)\n",
    "            accuracy = multiclass_accuracy(pred, val_y)\n",
    "            if accuracy > best_val_accuracy:\n",
    "                best_classifier = classifier\n",
    "                best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.217000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
